# -*- coding: utf-8 -*-
"""ML HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6lhakt794Y-2OHGzG4bBwRnX5JyMr3K
"""

#A trial to implement basic gradient descent

import time
import numpy as np
from matplotlib import pyplot as plt

# Randomly generate data
N = 5000
x = np.random.uniform(size=(N, 2))

y = (x[:, 0]**2 + x[:, 1]**2 < 1).astype(np.float32)

# visualize the data
def plot(x, y):
  plt.figure()
  plt.scatter(x[y==1.0][:, 0], x[y==1.0][:, 1], s=2.0, c='r')
  plt.scatter(x[y==0.0][:, 0], x[y==0.0][:, 1], s=2.0, c='b')
plot(x, y)
#在圆圈里等于1 不在圆圈里等于0

# define model class
class logistic():
    def __init__(self, hidden_dim=100, input_dim=2):
        self.alpha = np.random.randn(input_dim, hidden_dim)  # random initialize hidden layer weights
        self.alpha_0 = np.random.randn(hidden_dim)        # random initialize hidden layer biases
        self.beta = np.random.randn(hidden_dim, 1) # random initialize output layer weight
        self.beta_0 = np.random.randn(1)   # random initialize output layer bias
        self.params = [self.alpha, self.alpha_0, self.beta, self.beta_0]
        self.velocity = [np.zeros_like(param) for param in self.params]

    def relu(self, z):
        # ReLU activation function
        return np.maximum(0, z)

    def relu_derivative(self, z):
        # Derivative of ReLU function
        return np.where(z > 0, 1, 0)

    def forward_and_backward(self, x, y, lr, train=True):
        # (TODO!!!) implement one step forward and gradient descent
        # In the forward pass, we make prediction of x;
        # In the backward pass, we update our parameter using gradient descent
        #   based on x and label y;
        #
        # x: input data features, in shape (N, 2)
        # y: label of the data, in shape (N)
        # lr: learning rate for gradient descent
        # train: whether we do training or just testing
        #  if train is set to True, then we are at the training stage, we will
        #   do both forward and gradient descent to update the parameter;
        #  if train is set to False, then we are at the testing stage, we only
        #   need to do forward pass; when train is set to False, we don't need
        #   to input label y and learning rate since they will not be used by
        #   the function
        #
        # return:
        #  p The predicted probability that x belong to label 1, should be in shape (N, 1) or (N,)
        def forward(x):
            s_h = x @ self.alpha + self.alpha_0
            h = self.relu(s_h)
            s_o = h @ self.beta + self.beta_0
            return s_o, h, s_h

        def backward(x, y, s_o, h, s_h):
            p = 1 / (1 + np.exp(-s_o))
            tmp1 = y[:, np.newaxis] - p
            # Output layer gradients
            d_beta = h.T @ tmp1
            d_beta_0 = np.sum(tmp1)
            # Hidden layer gradients
            tmp2 = tmp1 * self.relu_derivative(s_h) * self.beta.T
            d_alpha = x.T @ tmp2
            d_alpha_0 = np.sum(tmp2.T, axis=1)

            return d_alpha, d_alpha_0, d_beta, d_beta_0

        def sgd(lr, grads, momentum=0.9):
            for i in range(len(self.params)):
                self.velocity[i] = momentum * self.velocity[i] + lr * grads[i]
                self.params[i] += self.velocity[i]

        if train:
            s_o, h, sh = forward(x)
            grads = backward(x, y, s_o, h, sh)
            sgd(lr, grads)
            return s_o

        else:
            s_o, h, _ = forward(x)
            p = 1 / (1 + np.exp(-s_o))
            return p

class logistic():
    def __init__(self, hidden_dim=100, input_dim=2):
        self.alpha = np.random.randn(input_dim, hidden_dim)  # random initialize hidden layer weights
        # 产出了一个2*100的矩阵，每一个内容都服从标准正态分布
        self.alpha_0 = np.random.randn(hidden_dim)        # random initialize hidden layer biases
        self.beta = np.random.randn(hidden_dim, 1) # random initialize output layer weight
        #beta 是一个100*1的矩阵 matrix
        self.beta_0 = np.random.randn(1)   # random initialize output layer bias
        self.params = [self.alpha, self.alpha_0, self.beta, self.beta_0]
        self.velocity = [np.zeros_like(param) for param in self.params]

model = logistic()
print(model.alpha_0[:5])
len(model.alpha_0)



# split data into training and testing set
idx = np.arange(5000)
np.random.shuffle(idx)
idx1 = idx[:4500]
idx2 = idx[4500:]
x_train, y_train = x[idx1], y[idx1]
x_test, y_test = x[idx2], y[idx2]

x[[1,2,3,4]]

idx1

# define models and training parameters
N_iter = 5000
lr = 1e-4
model = logistic()
threshold = 0.5

# training loop
start_time = time.time()

for i in range(N_iter):
    # (TODO!!!) Do 1 step forward and gradient descent to update the parameter
    # batch_x = x_train[i, :].reshape((1,-1))
    # batch_y = np.array([y_train[i]])
    # s_i = model.forward_and_backward(batch_x, batch_y, lr)
    s_i = model.forward_and_backward(x_train, y_train, lr)

    if i % 100 == 0:
        # calculate training and testing accuracy
        # neg_crossentropy = np.dot(batch_y[:, np.newaxis].T, s_i)[0][0] - np.sum(np.log(1 + np.exp(s_i)))
        neg_crossentropy = np.dot(y_train[:, np.newaxis].T, s_i)[0][0] - np.sum(np.log(1 + np.exp(s_i)))

        train_pred = (model.forward_and_backward(x_train, y_train, lr=0.0, train=False) > threshold ).astype(np.float32)
        train_acc = (train_pred.round() == y_train).astype(np.float32).mean().item()

        test_pred = (model.forward_and_backward(x_test, y_test, lr=0.0, train=False) > threshold ).astype(np.float32)
        test_acc = (test_pred.round() == y_test).astype(np.float32).mean().item()
        print('Iter {}, time {:.2f} neg_crossentropy {:.2f} train_acc {:.3f} test_acc {:.3f}'\
            .format(i, time.time() - start_time,  neg_crossentropy, train_acc, test_acc))

y_train

# plot decision boundary
x_plot = np.random.uniform(low=-1.0, high=1.0, size=(10000, 2))
x_pred = (model.forward_and_backward(x_plot, np.ones(len(x_plot)), lr=0.0, train=False).squeeze() > 0.5).astype(np.float32)
plot(x_plot, x_pred)
plt.savefig("classification.png")

# plot decision boundary
x_plot = np.random.uniform(low=-1.0, high=1.0, size=(10000, 2))
x_pred = (model.forward_and_backward(x_plot, np.ones(len(x_plot)), lr=0.0, train=False).squeeze() > 0.5).astype(np.float32)
plot(x_plot, x_pred)
plt.savefig("classification.png")

np.ones(len(x_plot))

x_plot = np.random.uniform(low=-1.0, high=1.0, size=(10000, 2))
x_pred = (model.forward_and_backward(x_plot, np.ones(len(x_plot)), lr=0.0, train=False).squeeze() > 0.5).astype(np.float32)