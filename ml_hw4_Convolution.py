# -*- coding: utf-8 -*-
"""Homework4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m8T-i9nqHoA2IdpOCUJqb_oJFzzJHFXN
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision

"""## This is where we download and use the dataset CIFAR10.

"""

transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

import os

# See the working directory to trace
print("Current working directory:", os.getcwd())

# see the file content of /data
print("Data folder contents:", os.listdir('./data'))

batch_size = 32

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)

class CIFAR10Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1)
        self.act1 = nn.ReLU()
        self.drop1 = nn.Dropout(0.3)

        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=1, padding=1)
        self.act2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))

        self.flat = nn.Flatten()

        self.fc3 = nn.Linear(8192, 512)
        self.act3 = nn.ReLU()
        self.drop3 = nn.Dropout(0.5)

        self.fc4 = nn.Linear(512, 10)

    def forward(self, x):
        # input 3x32x32, output 32x32x32
        x = self.act1(self.conv1(x))
        x = self.drop1(x)
        # input 32x32x32, output 32x32x32
        x = self.act2(self.conv2(x))
        # input 32x32x32, output 32x16x16
        x = self.pool2(x)
        # input 32x16x16, output 8192
        x = self.flat(x)
        # input 8192, output 512
        x = self.act3(self.fc3(x))
        x = self.drop3(x)
        # input 512, output 10
        x = self.fc4(x)
        return x

"""super().__init__(self) is to inherit the functions by the father's calss.  Self.Conv1 is to establish the first layer. Input is 3, and output is 32. kernel_size is 3,3, and stride is 1. The padding is what fulfills the outerframe is 1. \\
Act1 is the first activation layer. \\
drop1 is the first dropout layer to cancel 30% of the neurons to avoid overfitting. \\

fc4对应 CIFAR-10 数据集的 10 个类别，用于分类输出。

The batch_size refers to the size of the batch to train or test the data. \\
每个批次的大小，由 batch_size 参数控制。batch_size 的值通常是一个整数，在这里是32，表示每次从数据集中提取32个样本。
"""

import matplotlib.pyplot as plt
import numpy as np

# Get the data of a certain batch
dataiter = iter(trainloader)
images, labels = next(dataiter)

#Define a function to show the picture
def imshow(img):
    img = img / 2 + 0.5  # 逆归一化，将像素值恢复到0-1之间
    npimg = img.numpy()  # transform into numpy array
    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # adjust the dimensions
    plt.show()

# Show pictures
imshow(images[0])  # To show the first picture
print('Label:', labels[0].item())  # Print the relavant labels

model = CIFAR10Model()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

n_epochs = 20
for epoch in range(n_epochs):
    for inputs, labels in trainloader:
        # forward, backward, and then weight update
        y_pred = model(inputs)
        loss = loss_fn(y_pred, labels)
        optimizer.zero_grad() # This line is specifically to erase the previous accumulated gradient.
        loss.backward()#to start to get the gradient
        optimizer.step()

    acc = 0
    count = 0
    for inputs, labels in testloader:
        y_pred = model(inputs)
        acc += (torch.argmax(y_pred, 1) == labels).float().sum()
        count += len(labels)
    acc /= count
    print("Epoch %d: model accuracy %.2f%%" % (epoch, acc*100))

acc += (torch.argmax(y_pred, 1) == labels).float().sum()

acc

"""Epoch is like the training dataset size that we split our training into such different groups.
This code trains a neural network model on the CIFAR-10 dataset using SGD(Stochastic gradient descent) and calculates its accuracy on a test set after each epoch. The model’s weights are updated batch by batch, and the accuracy is displayed after every epoch to monitor the model's progress over time.
"""

from google.colab import drive
drive.mount('/content/drive')
file_path = '/content/drive/My Drive/Colab Notebooks/413 Machine learning/wonderland.txt'

with open(file_path, 'r',encoding = 'UTF-8') as file:
    content = file.read()
    print(content[800:1500])

import numpy as np
content = content[700:] #remove the illustration of the project

raw_text = content.lower()

# create mapping of unique chars to integers
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))

# summarize the loaded data
n_chars = len(raw_text)
n_vocab = len(chars)
print("Total Characters: ", n_chars)
print("Total Vocab: ", n_vocab)

"""This is the total characters the total vocabulary used in the fiction. So here we can see that it is a quite short book."""

# prepare the dataset of input to output pairs encoded as integers
seq_length = 100
dataX = []
dataY = []
for i in range(0, n_chars - seq_length, 1):
    seq_in = raw_text[i:i + seq_length]
    seq_out = raw_text[i + seq_length]
    dataX.append([char_to_int[char] for char in seq_in])
    dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)
print("Total Patterns: ", n_patterns)

import torch
import torch.nn as nn
import torch.optim as optim

# reshape X to be [samples, time steps, features]
X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)
X = X / float(n_vocab)
y = torch.tensor(dataY)
print(X.shape, y.shape)

import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data

class CharModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        self.linear = nn.Linear(256, n_vocab)
    def forward(self, x):
        x, _ = self.lstm(x)
        # take only the last output
        x = x[:, -1, :]
        # produce output
        x = self.linear(self.dropout(x))
        return x

char_to_int

n_epochs = 10
batch_size = 128
model = CharModel()

optimizer = optim.Adam(model.parameters())
loss_fn = nn.CrossEntropyLoss(reduction="sum")
loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)

best_model = None
best_loss = np.inf
for epoch in range(n_epochs):
    model.train()
    for X_batch, y_batch in loader:
        y_pred = model(X_batch)
        loss = loss_fn(y_pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    # Validation
    model.eval()
    loss = 0
    with torch.no_grad():
        for X_batch, y_batch in loader:
            y_pred = model(X_batch)
            loss += loss_fn(y_pred, y_batch)
        if loss < best_loss:
            best_loss = loss
            best_model = model.state_dict()
        print("Epoch %d: Cross-entropy: %.4f" % (epoch, loss))

torch.save([best_model, char_to_dict], "single-char.pth")

# Generation using the trained model
best_model, char_to_int = torch.load("single-char.pth")
n_vocab = len(char_to_int)
int_to_char = dict((i, c) for c, i in char_to_int.items())
model.load_state_dict(best_model)

# randomly generate a prompt
filename = "wonderland.txt"
seq_length = 100
raw_text = open(filename, 'r', encoding='utf-8').read()
raw_text = raw_text.lower()
start = np.random.randint(0, len(raw_text)-seq_length)
prompt = raw_text[start:start+seq_length]
pattern = [char_to_int[c] for c in prompt]

model.eval()
print('Prompt: "%s"' % prompt)
with torch.no_grad():
    for i in range(1000):
        # format input array of int into PyTorch tensor
        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)
        x = torch.tensor(x, dtype=torch.float32)
        # generate logits as output from the model
        prediction = model(x)
        # convert logits into one character
        index = int(prediction.argmax())
        result = int_to_char[index]
        print(result, end="")
        # append the new character into the prompt for the next iteration
        pattern.append(index)
        pattern = pattern[1:]
print()
print("Done.")

"""torch.load("single-char.pth") loads the best model weights (best_model) and a dictionary (char_to_int) that maps each character to a unique integer. n_vocab = len(char_to_int) determines the vocabulary size, i.e., the total number of unique characters.This code loads a trained character-level model and uses a random prompt to generate a new text sequence. \\
This model predicts the next character based on the current sequence, appending each generated character to continue generating text iteratively.

## Compared with the sample code, I experiment with a smaller amount of epochs and bigger batch size to accelerate the efficiency. I found that my training result is not as good as the sample code with a bigger cross-entropy.
"""